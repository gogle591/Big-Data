{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet Big Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsFRTcmrwN2o"
      },
      "source": [
        "\n",
        "\n",
        "<span style=\"color:red\">**Team members / emails**</span> --> * BIG DATA*\n",
        "- (1) MELLAL Houdaifa / hodaifa.mellal@gmail.com \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF341tTI5osH"
      },
      "source": [
        "# 1.Initialisation de l'environnement de travail:\n",
        "\n",
        "Dans cette partie, on a initialisé l'envrionnement spark avec la version 3.0.2 et hadoop 3.2, une installation de tout les package utilisé a également été fait."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKeRy7iUsn_B"
      },
      "source": [
        "##mise en place de l'environnement de travail \n",
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark3.0.0\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop3.2.tgz  \n",
        "# unzip it\n",
        "!tar xf spark-3.0.2-bin-hadoop3.2.tgz \n",
        "# install findspark \n",
        "!pip install -q findspark\n",
        "!pip install fastavro\n",
        "!pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTkj4-5A6MJi"
      },
      "source": [
        "Dans cette partie, on a importé et intialiser l'envrionnement avec spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcPJKR1xtCDa"
      },
      "source": [
        "# Set up required environment variables\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.0.1 pyspark-shell'\n",
        "\n",
        "findspark.init(\"spark-3.0.2-bin-hadoop3.2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBVqGure6SyH"
      },
      "source": [
        "Dans cette partie, on a lancé spark, en utilisant SQLContext, SPARKContext et SparkSession:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD2tUvRTuA4B"
      },
      "source": [
        "##Initation de Spark:\n",
        "\n",
        "from pyspark.sql import SQLContext # For the connexion with spark\n",
        "from pyspark import SparkContext # For the connexion with spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext('local', 'Spark SQL') \n",
        "sqlc = SQLContext(sc)\n",
        "spark = SparkSession.builder.appName('BIGDATA').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEWH7pqp6cVd"
      },
      "source": [
        "Téléchargement des données qu'on a utilisé durant le projet, il sera stocké dans le repertoire local du notebook. Le fichié téléchargé été compréssé, donc on a utilisé des commandes SHELL afin de lde décompresser:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9dvpqN4G-Z"
      },
      "source": [
        "!wget -q  http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz\n",
        "!tar -xf 20news-19997.tar.gz  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zk7o8G56mH3"
      },
      "source": [
        "La création des RDDs demandé alt.atheism et rec.sport.baseball, ceci est fait a partir des fichiers text, on a utilisé wholeTextFiles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUr_Rlqc4NBO"
      },
      "source": [
        "#Create the RDDs from alt.atheism and rec.sport.baseball files:\n",
        "\n",
        "RDD1 = sc.wholeTextFiles(\"20_newsgroups/alt.atheism/*\")\n",
        "RDD2= sc.wholeTextFiles(\"20_newsgroups/rec.sport.baseball/*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ia7PXVT63L5"
      },
      "source": [
        "La fonction extract_info est la fonction responsable de explorer les fichiers qui sont stocké dans des RDD, et puis recupèrer uniquement les informations qu'on a besoin, et qu'on va utiliser:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nRXf2GwOF0V"
      },
      "source": [
        "#The function that extract the informations:\n",
        "\n",
        "def extract_info(splited_elm):\n",
        "  i = 0\n",
        " \n",
        "  #Search for the end of header:\n",
        "  while( i< len(splited_elm) and not splited_elm[i].startswith('Lines:') ):\n",
        "    i+=1\n",
        "  if i == len(splited_elm):\n",
        "    return None\n",
        "  #Decompose the mail into header and body:\n",
        "  header = splited_elm[0:i+1]\n",
        "  body   = '\\n'.join(splited_elm[i+1:]).strip()\n",
        "  header_dict = {'Category':None,'Subject':None, 'Source' : None, 'Organization' : None, 'Lines':None,'Date':None}\n",
        "  \n",
        "  #Extract the informations from the header:\n",
        "  for header_elm in header:\n",
        "\n",
        "    if (header_elm.lower().startswith('newsgroups')):\n",
        "      header_dict['Category'] = header_elm[11:].strip()\n",
        "    \n",
        "    elif (header_elm.lower().startswith('subject')):\n",
        "      header_dict['Subject'] = header_elm[9:].strip()\n",
        "\n",
        "    elif (header_elm.lower().startswith('from')):\n",
        "      header_dict['Source'] = header_elm[5:].strip()\n",
        "    \n",
        "    elif (header_elm.lower().startswith('organization')):\n",
        "      header_dict['Organization'] = header_elm[13:].strip()\n",
        "    \n",
        "    elif (header_elm.lower().startswith('lines')):\n",
        "      header_dict['Lines'] = header_elm[6:].strip()\n",
        "\n",
        "    elif (header_elm.lower().startswith('date')):\n",
        "      header_dict['Date'] = header_elm[5:].strip()\n",
        "\n",
        "  return (header_dict,body)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqsz2TYV7DC5"
      },
      "source": [
        "A partir des RDD, et a l'aide de la fonction extract_info on a créer des RDDS avec uniquement les informations qu'on va utiliser, on a également fait un union entre les deux RDD et on a supprimer les lignes qui contiennent des valeurs NULL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JIFuGLuppr-"
      },
      "source": [
        "# Apply the traitement to the RDDs:\n",
        "\n",
        "RDD1 = RDD1.map(lambda x : extract_info(x[1].split('\\n')) )\n",
        "RDD2 = RDD2.map(lambda x : extract_info(x[1].split('\\n')) )\n",
        "\n",
        "# Merge the RDDs Objects:\n",
        "\n",
        "RDD = RDD1.union(RDD2) \n",
        "\n",
        "# Delete the None lines:\n",
        "\n",
        "RDD = RDD.filter(lambda x : x is not None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwu_44EB7bxf"
      },
      "source": [
        "Transformation de chaque des RDD en ROW format, ceci nous facilite la transformation en DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCPdxXSDS9Sq"
      },
      "source": [
        "#Tranform the document to Row format:\n",
        "from pyspark.sql import Row\n",
        "RDD = RDD.map(lambda x : Row(**x[0],Body = x[1]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hLTfVn7xVw"
      },
      "source": [
        "Transformation de la RDD qui regroupe les articiles des deux sujets en dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tx6YC_giLnq"
      },
      "source": [
        "#Create a DataFrame object from the RDD object\n",
        "Df = spark.createDataFrame(RDD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvfhv-UJ73Bz"
      },
      "source": [
        "sauvgarde de la DataFrame sous format Avro et Parquet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secGXTKitQ3k"
      },
      "source": [
        "# Save data on Avro and Parquet Formats: \n",
        "Df.write.format(\"avro\").save(\"Emailss.avro\")\n",
        "Df.write.parquet(\"Emailss.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uj-8lANSZzK"
      },
      "source": [
        "<h1>Analyse descriptive</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5johb0Y7_HG"
      },
      "source": [
        "Dans cette partie, on a explorer la dataframe, ceci afin de faciliter la préparation des données pour la phase de l'application de Kmeans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdOEC_Vh8Ujx"
      },
      "source": [
        "Affichage des nombre des organisations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXxMKo1sV-Wz"
      },
      "source": [
        "#Get the number of the different organizations: \n",
        "nbr_org = Df.select(\"Organization\").distinct().count()\n",
        "print(\"the number of the distinct organizations is : {}\".format(nbr_org))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U85xR-Y-8ZbI"
      },
      "source": [
        "Affichage des catégories afin de confirmer d'avoir deux principales catégories, ceci est remarquable en lisant les noms des catégories, ou chaque catéogrie a une relation avec le baseball ou bien l'athéisme:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzlQvGCpWgZw"
      },
      "source": [
        "# Show the distinct Categories to confirm that we have two principale categories:\n",
        "\n",
        "print(Df.select('Category').distinct().show(36))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcAGWVRQ8v56"
      },
      "source": [
        "Affichage d'une description du DataFrame, ceci nous permettra d'avoir les différentes informations concenant les differentes variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF4gG-MzTcQt"
      },
      "source": [
        "# Show the different variables of our data Frame\n",
        "print(\"The description of the data frame variables:\")\n",
        "print(Df.describe().show())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv-vURNd86-W"
      },
      "source": [
        "Affichage des 10 premiers lignes du dataFrame, ceci nous donnera une idée sur notre donnés:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PyxQAJfX877"
      },
      "source": [
        "#Show the first 10 rows : \n",
        "print(\"The first 10 rows:\")\n",
        "print(Df.show(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El8cp4549BUe"
      },
      "source": [
        "Affichage du nombre des lignes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pR2rnKtYEhx"
      },
      "source": [
        "#The number of rows : \n",
        "print('The number of mails in the dataset is: {}'.format((Df.count())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Ju6C4d9F8H"
      },
      "source": [
        "Affichage d'un resumé de notre DataSet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtBx2ebrZtzx"
      },
      "source": [
        "Df.summary().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqVerqxGa7rq"
      },
      "source": [
        "<h1>Transformation du texte et clustering</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Ias8Dg9JFK"
      },
      "source": [
        "Dans cette Partie, on a fais la transformation de text et le clustering en utilisant pyspark:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQ91a669Q-v"
      },
      "source": [
        "Dans cette partie, on a fait la tokenization des text, ceci consiste a séparer le text en un ensemble des mots. Après cela, on applique une supprision des stopwords ou ceci consiste a supprimer les mots qui n'ont pas vraiement un poids, et qui se repetent dans tout les sujets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5DOdS_uc5Ur"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer,StopWordsRemover\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"Body\", outputCol=\"Body_tokinized\")\n",
        "stopwordsremover = StopWordsRemover(inputCol='Body_tokinized', outputCol='Body_stopwords')\n",
        "ponctuation = StopWordsRemover(inputCol='Body_stopwords', outputCol='Body_words',stopWords=['?','!',',',';',':','(',')','[' ,']','«','»','–','{','}'])\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwordsremover,ponctuation])\n",
        "words = pipeline.fit(Df).transform(Df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4rGXidKB9yw"
      },
      "source": [
        "Application du hashingTF qui consiste a hasher chaque mot et lui assister le nombre de répitions, on a spécifier le paramètre 2000 afin de limiter les variables a 2000 mots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4xZwihNRSMI"
      },
      "source": [
        "hashingTF = HashingTF(numFeatures = 2000 ,inputCol=\"Body_words\", outputCol=\"rawFeatures\")\n",
        "wordsData = hashingTF.transform(words).na.drop('any')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcGqLDR2DcY7"
      },
      "source": [
        "Application de TF-IDF en appuiyant sur le résultat de HASHINGTF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kpv2k-HSwVY"
      },
      "source": [
        "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
        "idfModel = idf.fit(wordsData)\n",
        "rescaledData = idfModel.transform(wordsData)\n",
        "rescaledData.select(\"features\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmeSXOS-DjW8"
      },
      "source": [
        "Dans cette partie on a appliqué l'algorithme Kmeans sur les données résultats de TF-IDF :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwnn_Q0HfzCL"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol='features').setK(2).setSeed(1)\n",
        "\n",
        "# Make predictions\n",
        "model = kmeans.fit(rescaledData)\n",
        "model\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(rescaledData)\n",
        "\n",
        "predictions.select(\"prediction\").distinct().count()\n",
        "predictions.select(\"Category\",\"prediction\")\n",
        "predictions.summary().show()\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator(featuresCol='rawFeatures')\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf7XQ3eCDqj8"
      },
      "source": [
        "La récupération des labels des prédits par l'algorithme de Kmeans de PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWeWfSn5ztDR"
      },
      "source": [
        "import numpy as np\n",
        "pred = predictions.select('prediction').collect()\n",
        "resl = np.array([])\n",
        "for elm in pred:\n",
        "  resl = np.append(resl, elm[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBMY8BK_Ecik"
      },
      "source": [
        "Création d'un vecteur qui regroupe les classes des documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6swA4XvHoSHe"
      },
      "source": [
        "y = np.zeros(1877)\n",
        "y[1000:] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgVYCxa2Ek3K"
      },
      "source": [
        "La récupération des données utilisé pour chaque document dans l'algorithme de Kmeans, ceci est dans le but de réappliquer l'algorithme de Kmeans avec Sickit Learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diuNO1dO4D70"
      },
      "source": [
        "x = np.array(rescaledData.select('features').collect())\n",
        "X = np.array([])\n",
        "for elm in x :\n",
        "  X = np.append(X,elm[0])\n",
        "\n",
        "X = X.reshape(1877,2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7NOaDcnEuGq"
      },
      "source": [
        "Application du Kmeans avec Sickit learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWqjSF4n3fed"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asoaPHuxExPv"
      },
      "source": [
        "Calcule de NMI pour la classification de PySPark:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVTKkou95HFK"
      },
      "source": [
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "\n",
        "normalized_mutual_info_score(resl,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX-DiQ6mE6Ok"
      },
      "source": [
        "Calcule de NMI pour la classification de Sickit Learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxpnwBot5ZBA"
      },
      "source": [
        "normalized_mutual_info_score(kmeans.labels_, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obZjZUsRE9Nh"
      },
      "source": [
        "On peux remarquer l'implimentation de Pyspark est beaucoup meilleur que celle de SickitLeanr en terme des résultats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEr0v5hL84aK"
      },
      "source": [
        "<h1>Implémentation de K-means unidimensionnel</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1MMA8suGUfa"
      },
      "source": [
        "Dans cette partie, on a implimenté la K-Means unidmensionnel:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IQ9H3Z0GYlo"
      },
      "source": [
        "On a commencé par l'implimentation de la fonction qui calcule les centroides de chaque cluster, ceci est fait grace a des fonctions sur les RDD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoUv8AUn_8Ij"
      },
      "source": [
        "def compute_centroids(points,cluster_ids):\n",
        "  # ( Id_Cluster, Sum of element in that cluster ) \n",
        "  sum_by_cluster = cluster_ids.zip(points).groupByKey().mapValues(sum)\n",
        "\n",
        "  # (Id_Cluster, Number of occurence)\n",
        "  count_by_cluster_id = cluster_ids.zip(points).groupByKey().mapValues(len)\n",
        "\n",
        "  # ( Id_Cluster, Mean):\n",
        "  return count_by_cluster_id.join(sum_by_cluster).map(lambda x : (x[0],x[1][1]/x[1][0])).sortByKey()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWLI4bAGgsP"
      },
      "source": [
        "Dans la deuxième phase, on a implimenté la fonction assign clusters qui assigne chaque point a un cluster, ceci se base sur la distance carré qui est implimenté dans la fonction squared sitances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV9EuxxM0sfx"
      },
      "source": [
        "import numpy as np\n",
        "def squared_distances(point, centroids):\n",
        "  return np.sqrt(np.power(centroids-point,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq8N-HxT3D-O"
      },
      "source": [
        "def assign_clusters(points, centroids):\n",
        "  list_centr = np.array(centroids.map(lambda x: x[1]).collect())\n",
        "  return points.map(lambda x : np.argmin(squared_distances(x,list_centr)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX1NlNYCGvT6"
      },
      "source": [
        "Dans cette Partie, on a utilisé les fonctions déja défini, et on a implimenté l'algorithme de Kmeans Unidimensionnel, il prend en paramètres les points et le nombre des clusters, il choisi au hasard k points, et il applique le principe de kmeans jusqu'a la convergence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW2Dcyu9Bi-i"
      },
      "source": [
        "def KmeansUniDim(points,k):\n",
        "  centroids = np.array(points.collect()[0:k])\n",
        "  clusters_ids = points.map(lambda x : np.argmin(squared_distances(x,centroids))).collect()\n",
        "  new_list_id_clusters = []\n",
        "  #1st itiration:  \n",
        "  centroids = compute_centroids(points,sc.parallelize(clusters_ids))\n",
        "  new_cluster_ids = assign_clusters(points, centroids).collect()\n",
        "  #The stop condition:\n",
        "  while(not np.array_equal(new_cluster_ids,clusters_ids) ):\n",
        "    \n",
        "    clusters_ids = new_cluster_ids\n",
        "    centroids = compute_centroids(points,sc.parallelize(clusters_ids))\n",
        "    new_cluster_ids = assign_clusters(points, centroids).collect()\n",
        "  \n",
        "  return clusters_ids,centroids\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp3JHHXASXJZ"
      },
      "source": [
        "#Implémentation de K-means multidimensionnel\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A_n1sRQG_FD"
      },
      "source": [
        "Dans cette partie, on s'est basé sur la première implimentation du Kmeans pour pouvoir implimenter le Kmeans multidimensionnel, le meme principe a été utilisé, avec quelque modifications sur les dimensions.\n",
        "\n",
        "La fonction calculate_sum calcule la somme des dimensions afin de pouvoir calculer la variable sum_by_cluster. \n",
        "\n",
        "La fonction squared_distances_multiple est responsable de retourner les distance entre le point et chaque cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAMBN0f8FO3G"
      },
      "source": [
        "def calculate_sum(X):\n",
        "  sum = np.zeros(len(X[0]))\n",
        "  for elm in X:\n",
        "    sum = np.add(sum,elm)\n",
        "  return sum.astype(int)\n",
        "\n",
        "\n",
        "\n",
        "def compute_centroids_multidimensionnel(points,cluster_ids):\n",
        "  # ( Id_Cluster, Sum of element in that cluster ) \n",
        "  sum_by_cluster = cluster_ids.zip(points).groupByKey().map(lambda x : (x[0], list(x[1]))).mapValues(calculate_sum)\n",
        "\n",
        "  # (Id_Cluster, Number of occurence)\n",
        "  count_by_cluster_id = cluster_ids.zip(points).groupByKey().mapValues(len)\n",
        "\n",
        "  # ( Id_Cluster, Mean):\n",
        "  return count_by_cluster_id.join(sum_by_cluster).map(lambda x : (x[0],x[1][1]/x[1][0])).sortByKey()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def assign_clusters_multidimensionnel(points, centroids):\n",
        "  list_centr = np.array(centroids.map(lambda x: x[1]).collect())\n",
        "  return points.map(lambda x : np.argmin(squared_distances_multiple(np.array(x),list_centr)))\n",
        "\n",
        "def squared_distances_multiple(point, centroids):\n",
        "    return np.sqrt(np.sum(np.power(centroids-point,2),axis=1))\n",
        "  \n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ_I2rLNI_o_"
      },
      "source": [
        "Cette fonction s'appuie sur les fonctions déja défini afin de donner une implimentation du kmeans multidimentionnel, l'intialisation est également random selon le nombre des clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd6b0KWLiCge"
      },
      "source": [
        "def KmeansMultiDim(points,k):\n",
        "  deb = np.random.randint(len(points.collect())-k)\n",
        "  centroids = np.array(points.collect()[deb:deb+k])\n",
        "  clusters_ids = points.map(lambda x : np.argmin(squared_distances_multiple(x,centroids))).collect()\n",
        "  new_list_id_clusters = []\n",
        "  \n",
        "  #1st itiration:  \n",
        "  centroids = compute_centroids_multidimensionnel(points,sc.parallelize(clusters_ids))\n",
        "  new_cluster_ids = assign_clusters_multidimensionnel(points, centroids).collect()\n",
        "  #The stop condition:\n",
        "  while(not np.array_equal(new_cluster_ids,clusters_ids) ):\n",
        "    \n",
        "    clusters_ids = new_cluster_ids\n",
        "    centroids = compute_centroids_multidimensionnel(points,sc.parallelize(clusters_ids))\n",
        "    new_cluster_ids = assign_clusters_multidimensionnel(points, centroids).collect()\n",
        "  \n",
        "  return clusters_ids,centroids\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s6PZG2qWGs-"
      },
      "source": [
        "points = sc.parallelize([[1,2,3],[1,3,5],[1,5,9]])\n",
        "clusters_ids = sc.parallelize([0,1,0]) \n",
        "\n",
        "sum_by_cluster = clusters_ids.zip(points).groupByKey().map(lambda x : (x[0], list(x[1]))).mapValues(calculate_sum)\n",
        "\n",
        "count_by_cluster_id = clusters_ids.zip(points).groupByKey().mapValues(len)\n",
        "\n",
        "count_by_cluster_id.join(sum_by_cluster).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gozoZ6JAQPAt"
      },
      "source": [
        "#Implémentation de Spherical K-Means\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C0SpmhHJa6a"
      },
      "source": [
        "L'implémentation de SPherical K-Means est également du meme principe que l'implimentation de K-Means multidimension, la seule difference c'est que Spherical Kmeans s'appuiue sur la distance cosianne, ceci est fait a l'aide du package scipy et la fonction spatial, donc SKeamns s'appuie plutot sur le notion de profil\n",
        "\n",
        "assignation des clusters s'appuie donc sur la fonction distance_cosine qui définit la distance cosianne entre deux individu ( en prendre en compte le cosinus de l'angle entre deux vecteur )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJgVrDH35vgk"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def assign_clusters_spherical(points, centroids):\n",
        "  list_centr = np.array(centroids.map(lambda x: x[1]).collect())\n",
        "  return points.map(lambda x : np.argmax(distance_cosine(np.array(x),list_centr)))\n",
        "\n",
        "def distance_cosine(X,list_centr):\n",
        "  distance = np.array([])\n",
        "  for center in list_centr:\n",
        "    distance = np.append(distance,1 - spatial.distance.cosine(X,center))\n",
        "  print('hello')\n",
        "  return distance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqoQx8SFJu8M"
      },
      "source": [
        "Comme les deux autres, cette fonction représente l'implimentation du sphérical Kmeans, elle s'appuie sur l'assignation des clusters spherical, et la distance cosine, ainsi que le calcule centroids multidimensionnel afin d'implimenter l'algorithme spherical Kmeans.\n",
        "\n",
        "L'initialisation est random en suivant la valeur k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfDOhaZn6vqE"
      },
      "source": [
        "def SKmeansMultiDim(points,k):\n",
        "  deb = np.random.randint(len(points.collect())-k)\n",
        "  centroids = np.array(points.collect()[deb:deb+k])\n",
        "  clusters_ids = points.map(lambda x : np.argmax(distance_cosine(x,centroids))).collect()\n",
        "  \n",
        "  new_list_id_clusters = []\n",
        "  #1st itiration:  \n",
        "  centroids = compute_centroids_multidimensionnel(points,sc.parallelize(clusters_ids))\n",
        "  new_cluster_ids = assign_clusters_spherical(points, centroids).collect()\n",
        "  #The stop condition:\n",
        "  while(not np.array_equal(new_cluster_ids,clusters_ids) ):\n",
        "    \n",
        "    clusters_ids = new_cluster_ids\n",
        "    centroids = compute_centroids_multidimensionnel(points,sc.parallelize(clusters_ids))\n",
        "    new_cluster_ids = assign_clusters_spherical(points, centroids).collect()\n",
        "  \n",
        "  return clusters_ids,centroids\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCggeemjRBFY"
      },
      "source": [
        "#Analyser les résultats:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpva12x3LPLm"
      },
      "source": [
        "Dans cette section, on analyse les résultats des différents implimentation des algorithme de kmeans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms2Mu9BQLZqj"
      },
      "source": [
        "### 1. Unidimensionnel Kmeans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imiW88E3Lv1J"
      },
      "source": [
        "On commence par définir 8 points, qui represent 3 clusters, le premier regroupe 1,0,-1 ( proche de 10) , le deuxième -20,-12,-15 ( largement inferieur a zero ) , et le troisième c'est 20,30 ( largement supérieur a 0 );"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55G9vV6BLgAG"
      },
      "source": [
        "points = sc.parallelize([1,-20,20,0,-1,-15,30,-12])\n",
        "labels, centroids =KmeansUniDim(points,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcHPrh36OQ5N"
      },
      "source": [
        "Le résultat est le suivant:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmesBcquL_y1"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yalvVUvwOWVB"
      },
      "source": [
        "Les centroids sont les suivant qui sous format de tuple, le premier represente le cluster, et le deuxième le centre de ce cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm6dhU37OPv9"
      },
      "source": [
        "centroids.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGYocK8pZCf-"
      },
      "source": [
        "### 2.SKmeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0eXjatOqrZ"
      },
      "source": [
        "Le deuxième exemple consiste a implimenté un Skmeans sur les données, ou cette exemple est constitué de deux cluster, le premier est celui des vecteur avec x positif, et les autres avec x négatifs. \n",
        "\n",
        "Notons que plusieurs intialisation peuvent conduire a plusiers résultats, donc il est préférable de relancer l'exécution plusieurs fois pour avoir les meilleurs résultats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR0kS69XRCuw"
      },
      "source": [
        "Multiple_points = sc.parallelize([[1, 1], [2, 2], [3, 3], [-1, 1], [-2, 2], [-3, 3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBlng12i-d4j"
      },
      "source": [
        "labels,Centeroids = SKmeansMultiDim(Multiple_points,2)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOUc0L9I-klC"
      },
      "source": [
        "Centeroids.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUGu3k5nQfhJ"
      },
      "source": [
        "### 3.Multidimensionel kmeans:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YclAEf6cQhid"
      },
      "source": [
        "Dans cette exemple, on s'intéresse a un cas de 3 dimensions avec 2 cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82wVDTuFQjxB"
      },
      "source": [
        "points = sc.parallelize([[1, 1,1], [2, 2,2], [3, 3,3], [-1, -1,-1], [-2, -2,-2], [-3, -3,-3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfNh26bGQsMW"
      },
      "source": [
        "labels,Centeroids = KmeansMultiDim(points,2)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9BVLXdZW8ZK"
      },
      "source": [
        "Centeroids.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ1ka3blZK6D"
      },
      "source": [
        "Dans cette exemple, on compare les résultats avec l'algorithme de Kmeans de sickit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uut9Iy_jROhw"
      },
      "source": [
        "X = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [-1, -1, -1], [-2, -2, -2], [-3, -3, -3]])\n",
        "labels_sickit = KMeans(n_clusters=2).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5kXHodeRcvR"
      },
      "source": [
        "labels_sickit.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KewxQNCYZRrx"
      },
      "source": [
        "On peux remarquer qu'on a exactement le meme résultat que le Kmeans de Sickit-Learn."
      ]
    }
  ]
}